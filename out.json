[
{
    "model": "auth.user",
    "pk": 1,
    "fields": {
        "password": "pbkdf2_sha256$150000$uBCVtZEGZQwW$BJfZbz2DsrLVM6RbP98a7uMkZwE4B7SvxHlZx4ajzOM=",
        "last_login": "2019-05-06T15:08:21.171Z",
        "is_superuser": true,
        "username": "dio",
        "first_name": "",
        "last_name": "",
        "email": "",
        "is_staff": true,
        "is_active": true,
        "date_joined": "2018-02-22T10:17:19.816Z",
        "groups": [],
        "user_permissions": []
    }
},
{
    "model": "base.school",
    "pk": 1,
    "fields": {
        "title": "Masaryk University, Faculty of Informatics in Brno",
        "started": "2014-09-01",
        "finished": "2016-01-01",
        "description": "Studied on Faculty of Informatics in Brno.",
        "major": "Information Systems"
    }
},
{
    "model": "base.school",
    "pk": 2,
    "fields": {
        "title": "Masaryk University, Faculty of Informatics in Brno",
        "started": "2011-09-01",
        "finished": "2014-09-01",
        "description": "Studied on Faculty of Informatics in Brno. Degree was completed in 2014.",
        "major": "Computer Systems and Data Processing"
    }
},
{
    "model": "base.project",
    "pk": 1,
    "fields": {
        "title": "Sonder - Data Warehouse / ETL System",
        "image": "projects/2c5d26718e9b4d14a07cfbff7a58205a.jpg",
        "description": "I was working with an US-based hospitality startup called Sonder as a Python Developer. \r\n\r\n\r\nTheir goal was to create a robust data pipeline able to collect from multitude of different data sources.\r\nIn the end, it was 43 different sources, some of them from open data platforms provided by the US government, some were web scraped.\r\nFrom the technical side, it was mostly my own effort to both architect and implement a flexible, robust and easily extensible data pipeline.",
        "url": "https://www.sonder.com",
        "from_date": "2017-07-01",
        "to_date": "2018-10-31"
    }
},
{
    "model": "base.project",
    "pk": 6,
    "fields": {
        "title": "Newchip - API for iOS app",
        "image": "projects/30f4c8f1c930498ab44580ab63699a35.jpg",
        "description": "Newchip is a FinTech startup that helps people to invest into other startups and I was hired as a Django API developer there.\r\n\r\n\r\nNewchip's main focus was developing an iOS app and I was responsible for the API built on top of Django (DRF) running on Heroku. \r\nOne of the efforts I was a part of was creating a dashboard for paid users.\r\nWe built a dashboard which allowed the user (owner of a startup) to see analytics about his funding deal. \r\nThe dashboard also allowed the user to edit information about his deal and it was integrated with Chargebee subscription system.\r\n\r\n\r\nI've learned a lot from this job in terms of agile software development, team work and leadership.\r\nIt has given me a chance to grow professionally and to work with some very talented people.",
        "url": "https://newchip.com/",
        "from_date": "2018-07-01",
        "to_date": "2019-05-06"
    }
},
{
    "model": "base.project",
    "pk": 7,
    "fields": {
        "title": "IntrospectData - Data Ingestion Toolset",
        "image": "projects/36cec7a1ad3b4ba788b074d6ba7448c5.jpg",
        "description": "IntrospectData is a Data Science startup and I worked with them as a Flask API developer.\r\n\r\n\r\nI was given a set of Swagger documents detailing multiple APIs that were designed to work in a microservice manner.\r\nMy responsibility was then to implement these specs in Flask with the use of MongoDB.\r\n\r\n\r\nI had a chance to work with a very talented, experienced developer, who has shown me a lot from DevOps, automation and CI/CD systems. \r\nThis job has been one of my favorite so far, simply because of the amount of fun I had learning and working on this.",
        "url": "https://introspectdata.com",
        "from_date": "2017-07-01",
        "to_date": "2018-10-31"
    }
}
]
